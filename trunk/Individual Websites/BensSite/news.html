<!DOCTYPE html>
<html>
  <head>
    <title>CUDA Squeeze: A predictive compression scheme</title>
    <meta content="">
    <link rel="stylesheet" type="text/css" href="css/bensTheme.css">
  </head>
  <body>
	  <div id="header">
		  <table id="headerimages">
			<tbody>
				<tr>
					<td id="logoCol">
					<img id="logo" src="img/logo.png" alt="Cuda Squeeze logo"/></br>
					<i id="subtitle">Implementation of a predictive scheme for compression of Radio Telescope Data</i>
					</td>
					<td id="skaLogoCol">
					<a href="http://www.ska.ac.za"><img id="skaLogo" src="img/ska.square.logo.png" alt="SKA Project"></img></a>
					</td>
				</tr>
			</tbody>
		  </table>
	  </div>
		<div id="links">
		<table align="center">
			<tbody>
				<tr>
				<td><a class="link" href="index.html">Overview</a></td>
				<td><a class="linkSelected">News</a></td>
				<td><a class="link" href="results.html">Results</a></td>
				<td><a class="link" href="resources.html">Resources</a></td>
				</tr>
			</tbody>
		</table>
	  </div>
		<div id="content">
		<h1>News</h1>
		<h1 class="newsfeed">12 July 2013</h1>
		<p>I've made a lot of progress since the last update on this feed. I've added some assembler to do the leading zero counting
		and that alone give me a couple of hundred MB/s speedup. I'm pleased to report that on the larger files I get up to 
		5 GB/s on the compressor and over 9 GB/s when there are no other processes making active use of the memory. On the files with smaller
		timestamps I don't get the same performance with the compressor only achieving about 3.7GB/s.</p>
		<p>I then went and added on an SSE 4.2 + AMD XOP version of both the compressor and the decompressor. This proved a fruitless avenue. 
		I actually recorded a massive slowdown in both the compressor and decompressor on the larger 850MB sample. According to comments online having 
		many set operations actually inflates the number of assembly instructions. This is the best explanation I could come up with to date.</p>
		<p>Depending on my supervisor's concent and access to the new Intel Haswell architecture I may still try out an AVX2 version of the code.
		Converting the SSE + XOP version to AVX2 should not take very long, since the instruction sets are very similar. However, it is my opinion
		that we should move on to doing a CUDA version now, to see if running a similar kernel on the GPU will give better results.</p>
		<h1 class="newsfeed">4 July 2013 (Part 2)</h1>
		<p>After I've replaced pthreads with OMP and done some more optimizations I've finally achieved a very good throughput
		on both the compressor and decompressor. My compressor now runs at ~1.98 GB/s and the decompressor runs at ~3.66GB/s using
		16 cores. The code is now ready to write some SSE &amp; AVX instructions that will hopefully get me the required throughput.
		If not we will have to try out the CUDA route in order to get the magical 40 GiB we're aiming for.</p>
		<h1 class="newsfeed">3-4 July 2013</h1>
		<p>Turns out that I will need the SSE4.2 &amp; AVX2 instruction sets to facilitate the operations I require, including
		elementwise bitshifting. I've also decided to do away with the atomic operations and the parallel prefix sum by cutting up the
		data instead making the compression algorithm parallel. Prof. Gain agreed with this approach. After a long night of restructuring
		I've managed a significant increase in performance. I now get up to
		0.673 GB/s on the compressor and 1.178 GB/s on the decompressor using 16 cores. I will look into further optimizations later today, including 
		vectorized instructions. But first things first: need to grab a few hours sleep :(</p>
		<h1 class="newsfeed">30 June 2013 (Part 2)</h1>
		<p>For the sake of thoroughness I've decided to try out compaction at a byte level instead of a bit level and I'm seemingly
		saving approximately 1% in compression ratio. This is likely due to the fact that the prefixes can be represented by 2 bits
		instead of 5 bits. This also meant I could remove one of the branches for saving prefixes. Now I'm getting approximately ~400 MB/s
		on the compressor and ~600 MB/s on the decompressor. I think further optimizations are now possible on the prefix loop since 4 prefixes
		can be saved simultaniously.</p>
		<h1 class="newsfeed">30 June 2013</h1>
		<p> I've added some further optimizations to make use of the CPU instruction set to count leading zeros, as well as other minor
		tweeks including a bittwidled max function for 32 bit integers. I now get up to ~380 MB/s on the compressor and up to
		~450 MB/s on the decompressor.</p>
		<h1 class="newsfeed">28 June 2013</h1>
		<p>I've completed an OMP parallel implementation of the compressor &amp; decompressor. It seems the data
		compresses best if leading zeros up to 31 bits are compressed (I'm getting compression ratios between 6% and 7%).
		I'm hoping that by running the algorithm on a machine with more cores and by adding vectorized instructions I will 
		get a better speedup. Currently compression is completed at ~250 MB/s and decompression is arround ~400 MB/s. </p>
		<h1 class="newsfeed">18 June 2013</h1>
		<p>From preliminary observations of the 32 bit floating point MeerKAT data it seems that a predictive algorithm
		will result in Leading Zero Counts of about 12-13 bits (for the most part). This is not even two whole bytes! In light of
		this I've adjusted the scheme to pack bit and not byte residuals as described in the 64 bit schemes that I've investigated 
		earlier.</p>
		<p>To parallise this packing scheme (for bits and bytes alike) it is necessary to compute a prefix sum to calculate the starting 
		positions of each of the values as suggested by O'Neil et al. in their paper on a fast CUDA 64 bit compression scheme. Guy E. Blelloch
		(1990) suggests a parallel algorithm for achieving this. The algorithm requires that the array it operates on is padded up to a power of
		two because it uses a binary tree representation (and associated array indices in powers of two). I've implemented this scheme for
		the CPU version. The GPU version requires more work (specifically to avoid bank conflicts and processing over multiple blocks), but is 
		fully described in the book GPU Gems 3.</p>
		<h1 class="newsfeed">10 June 2013</h1>
		<p>First iteration of the website is now online. Algorithm development also starts today...</p>
		</div>
	  <div id="footer">
		  <i id="copyright">Page design by Benjamin Hugo, 2013</i>
	  </div>
  </body>
</html>
