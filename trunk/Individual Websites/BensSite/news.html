<!DOCTYPE html>
<html>
  <head>
    <title>CUDA Squeeze: A predictive compression scheme</title>
    <meta content="">
    <link rel="stylesheet" type="text/css" href="css/bensTheme.css">
  </head>
  <body>
	  <div id="header">
		  <table id="headerimages">
			<tbody>
				<tr>
					<td id="logoCol">
					<img id="logo" src="img/logo.png" alt="Cuda Squeeze logo"/></br>
					<i id="subtitle">Implementation of a predictive scheme for compression of Radio Telescope Data</i>
					</td>
					<td id="skaLogoCol">
					<a href="http://www.ska.ac.za"><img id="skaLogo" src="img/ska.square.logo.png" alt="SKA Project"></img></a>
					</td>
				</tr>
			</tbody>
		  </table>
	  </div>
		<div id="links">
		<table align="center">
			<tbody>
				<tr>
				<td><a class="link" href="index.html">Overview</a></td>
				<td><a class="linkSelected">News</a></td>
				<td><a class="link" href="results.html">Results</a></td>
				<td><a class="link" href="resources.html">Resources</a></td>
				</tr>
			</tbody>
		</table>
	  </div>
		<div id="content">
		<h1>News</h1>
		<h1 class="newsfeed">28 June 2013</h1>
		<p>I've completed an OMP parallel implementation of the compressor &amp; decompressor. It seems the data
		compresses best if leading zeros up to 31 bits are compressed (I'm getting compression ratios between 6% and 7%).
		I'm hoping that by running the algorithm on a machine with more cores and by adding vectorized instructions I will 
		get a better speedup. Currently compression is completed at ~250 MB/s and decompression is arround ~400 MB/s. </p>
		<h1 class="newsfeed">18 June 2013</h1>
		<p>From preliminary observations of the 32 bit floating point MeerKAT data it seems that a predictive algorithm
		will result in Leading Zero Counts of about 12-13 bits (for the most part). This is not even two whole bytes! In light of
		this I've adjusted the scheme to pack bit and not byte residuals as described in the 64 bit schemes that I've investigated 
		earlier.</p>
		<p>To parallise this packing scheme (for bits and bytes alike) it is necessary to compute a prefix sum to calculate the starting 
		positions of each of the values as suggested by O'Neil et al. in their paper on a fast CUDA 64 bit compression scheme. Guy E. Blelloch
		(1990) suggests a parallel algorithm for achieving this. The algorithm requires that the array it operates on is padded up to a power of
		two because it uses a binary tree representation (and associated array indices in powers of two). I've implemented this scheme for
		the CPU version. The GPU version requires more work (specifically to avoid bank conflicts and processing over multiple blocks), but is 
		fully described in the book GPU Gems 3.</p>
		<h1 class="newsfeed">10 June 2013</h1>
		<p>First iteration of the website is now online. Algorithm development also starts today...</p>
		</div>
	  <div id="footer">
		  <i id="copyright">Page design by Benjamin Hugo, 2013</i>
	  </div>
  </body>
</html>
