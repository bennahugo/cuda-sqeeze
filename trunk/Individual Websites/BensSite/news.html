<!DOCTYPE html>
<html>
  <head>
    <title>CUDA Squeeze: A predictive compression scheme</title>
    <meta content="">
    <link rel="stylesheet" type="text/css" href="css/bensTheme.css">
  </head>
  <body>
	  <div id="header">
		  <table id="headerimages">
			<tbody>
				<tr>
					<td id="logoCol">
					<img id="logo" src="img/logo.png" alt="Cuda Squeeze logo"/></br>
					<i id="subtitle">Implementation of a predictive scheme for compression of Radio Telescope Data</i>
					</td>
					<td id="skaLogoCol">
					<a href="http://www.ska.ac.za"><img id="skaLogo" src="img/ska.square.logo.png" alt="SKA Project"></img></a>
					</td>
				</tr>
			</tbody>
		  </table>
	  </div>
		<div id="links">
		<table align="center">
			<tbody>
				<tr>
				<td><a class="link" href="index.html">Overview</a></td>
				<td><a class="linkSelected">News</a></td>
				<td><a class="link" href="results.html">Results</a></td>
				<td><a class="link" href="resources.html">Resources</a></td>
				</tr>
			</tbody>
		</table>
	  </div>
		<div id="content">
		<h1>News</h1>
		<h1 class="newsfeed">3-4 July 2013</h1>
		<p>Turns out that I will need the SSE4.2 &amp; AVX2 instruction sets to facilitate the operations I require, including
		elementwise bitshifting. I've also decided to do away with the atomic operations and the parallel prefix sum by cutting up the
		data instead making the compression algorithm parallel. Prof. Gain agreed with this approach. After a long night of restructuring
		I've managed a significant increase in performance. I now get up to
		0.673 GB/s on the compressor and 1.178 GB/s on the decompressor using 16 cores. I will look into further optimizations later today, including 
		vectorized instructions. But first things first: need to grab a few hours sleep :(</p>
		<h1 class="newsfeed">30 June 2013 (Part 2)</h1>
		<p>For the sake of thoroughness I've decided to try out compaction at a byte level instead of a bit level and I'm seemingly
		saving approximately 1% in compression ratio. This is likely due to the fact that the prefixes can be represented by 2 bits
		instead of 5 bits. This also meant I could remove one of the branches for saving prefixes. Now I'm getting approximately ~400 MB/s
		on the compressor and ~600 MB/s on the decompressor. I think further optimizations are now possible on the prefix loop since 4 prefixes
		can be saved simultaniously.</p>
		<h1 class="newsfeed">30 June 2013</h1>
		<p> I've added some further optimizations to make use of the CPU instruction set to count leading zeros, as well as other minor
		tweeks including a bittwidled max function for 32 bit integers. I now get up to ~380 MB/s on the compressor and up to
		~450 MB/s on the decompressor.</p>
		<h1 class="newsfeed">28 June 2013</h1>
		<p>I've completed an OMP parallel implementation of the compressor &amp; decompressor. It seems the data
		compresses best if leading zeros up to 31 bits are compressed (I'm getting compression ratios between 6% and 7%).
		I'm hoping that by running the algorithm on a machine with more cores and by adding vectorized instructions I will 
		get a better speedup. Currently compression is completed at ~250 MB/s and decompression is arround ~400 MB/s. </p>
		<h1 class="newsfeed">18 June 2013</h1>
		<p>From preliminary observations of the 32 bit floating point MeerKAT data it seems that a predictive algorithm
		will result in Leading Zero Counts of about 12-13 bits (for the most part). This is not even two whole bytes! In light of
		this I've adjusted the scheme to pack bit and not byte residuals as described in the 64 bit schemes that I've investigated 
		earlier.</p>
		<p>To parallise this packing scheme (for bits and bytes alike) it is necessary to compute a prefix sum to calculate the starting 
		positions of each of the values as suggested by O'Neil et al. in their paper on a fast CUDA 64 bit compression scheme. Guy E. Blelloch
		(1990) suggests a parallel algorithm for achieving this. The algorithm requires that the array it operates on is padded up to a power of
		two because it uses a binary tree representation (and associated array indices in powers of two). I've implemented this scheme for
		the CPU version. The GPU version requires more work (specifically to avoid bank conflicts and processing over multiple blocks), but is 
		fully described in the book GPU Gems 3.</p>
		<h1 class="newsfeed">10 June 2013</h1>
		<p>First iteration of the website is now online. Algorithm development also starts today...</p>
		</div>
	  <div id="footer">
		  <i id="copyright">Page design by Benjamin Hugo, 2013</i>
	  </div>
  </body>
</html>
